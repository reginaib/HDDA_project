---
title: "HDDA-Project23"
author: "Regina Ibragimova"
date: "2023-12-09"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = TRUE) 
# Load the libraries required 
library(pls)
library(glmnet)
library(dplyr)
library(boot)
library(ggplot2)


#Load the data
load("breastCancerNKI.RData")

# Scale the gene data (X was already centered)
X_scaled <- scale(X, scale = TRUE)
```


# Part 1: Analysis of gene expression data

## Evaluation and comparison prediction of the models

In this step, the goal was to predict the ER status using gene expression levels. First, the data was randomly split into a a training (70%) and test (30%) datasets. We  evaluated Principal Component (PCR), Ridge Regression, and Lasso Regression models. The training dataset was utilized for model training, validation and hyperparameter tuning such as the number of Principal Components (PCs) for PCR and the regularization parameter $\lambda$ in the Ridge and Lasso models. The test data was used for accessing the performance of models.

```{r data split, echo = FALSE}
# Split the data into train and test
# Set seed for reproducibility, RNGkind is used to specify how sample() works
RNGkind(sample.kind = "Rounding")
set.seed(1234) 
n <- nrow(X)
nTrain <- round(0.7*n)
indTrain <- sample(n,nTrain)
XTrain <- X_scaled[indTrain,]
YTrain <- Y[indTrain]
XTest <- X_scaled[-indTrain, ]
YTest <- Y[-indTrain]
```
### Principal Component Regression
First, principal component analysis (PCA) was employed to reduce dimensionality of the gene expression data, resulting in 236 principal components (PCs). The training dataset was used to fit the model, and cross-validation was implemented to determine the optimal number of PCs, using the misclassification rate as the performance metric. A warning of 'fitted probabilities numerically 0 or 1 occurred' was observed when more than 13 PCs were used, suggesting potential overfitting of the model due to excessively good data separation. Through the cross-validation procedure, it was established that the first 7 principal components struck the best balance between capturing essential features of the gene expression data and accurately predicting the Estrogen Receptor (ER) status. 
The plot below visualizes the misclassification error rates resulting from the use of varying numbers of principal components in the predictive model.
A sharp decline in error is observed as the number of components increases from 1 to 3, suggesting a significant gain in predictive power with the inclusion of the second and third principal component. The error decreases further, albeit at a slower rate, until it reaches a minimum at 5.5% at 7 components. Beyond this point, adding more components does not substantially improve the model's performance, and in some cases, it slightly increases the error, indicating a potential overfit or that these additional components do not contribute meaningful information for the prediction. Consequently, we decided to further restrict the model to these 7 PCs. These components were then selected for subsequent analyses. This approach significantly reduced the complexity of the model while retaining the most informative aspects of the data for predicting ER status in breast cancer patients.

```{r pcr, results='hide', echo = FALSE}
# Principal Component Regression
## PCA first

pca_X <- prcomp(XTrain)
k <- 13
scores <- pca_X$x[,1:k]

pcr_data <- data.frame(YTrain, scores)
pcr_model <- glm(YTrain ~ ., data = pcr_data, family = "binomial")
#Warning messages:
#1: glm.fit: algorithm did not converge 
#2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
# the warning occurs only when k > 13 
summary(pcr_model)

# Function to specify the cost of cv.glm
misclass_error <- function(obs, pred = 0) mean(abs(obs-pred) > 0.5)

# Loop over PCs
set.seed(1234) 
cv.error <- rep(0, k)
for (i in 1:k) {
  pcr_data2 <- pcr_data[,1:(i+1)]
  pcr.cv <- cv.glm(data = pcr_data2,  
                   glmfit = pcr_model,
                   cost = misclass_error, 
                   K = 10)
  cv.error[i] <- pcr.cv$delta[1]
}
# Get the number of optimal PCs
(opt_PC <- which.min(cv.error))

# Create a data frame for plotting
pcr_data_plot <- data.frame(PC = 1:length(cv.error), Error = cv.error)

# Plotting
ggplot(pcr_data_plot, aes(x = PC, y = Error)) + 
  geom_line(size=1, color="#00BFC4") + 
  geom_point(size=3, shape=21, fill="black") +
  scale_x_continuous(breaks=1:length(cv.error)) +
  labs(title = "Misclassification Error for Number of Principal Components",
       x = "Number of Principal Components", 
       y = "Misclassification Error") +
  theme_minimal()

# Final PCR model
final_scores <- pca_X$x[,1:opt_PC]
final_pcr_data <- data.frame(YTrain, final_scores)
final_pcr_model <- glm(YTrain ~ ., data = final_pcr_data, 
                       family = "binomial")
```

## Ridge Regression
Next, Ridge Regression was applied to the training data as a regularized linear regression method to address potential overfitting. It incorporates an $L_2$  penalty term—squared magnitude of coefficients—into the loss function, regulated by the $\lambda$ parameter. During cross-validation, the goal was to find the $\lambda$ that yielded the lowest misclassification error. The optimal $\lambda$ was found to be 1.748. A $\lambda$ of 2.310, within one standard error of the optimal, was also considered as it corresponds to a simpler model that is expected to perform comparably to the model with the optimal lambda.

```{r ridge, results='hide', echo=FALSE}

# Model fitting 
Ridge <- glmnet(x = XTrain,
                y = YTrain,
                alpha = 0,
                family="binomial")

# Cross-validation
set.seed(1234) 
grid <- 10^seq(-1, 2,length = 100)
Ridge_CV <- cv.glmnet(x = XTrain,
                      y = YTrain,
                      alpha = 0,
                      type.measure = "class",
                      family = "binomial",
                      lambda = grid) 

plot(Ridge_CV)

plot(Ridge_CV$glmnet.fit, xvar = "lambda")
lambda <- Ridge_CV$lambda.min
text(log(lambda), -0.05, labels = expression(lambda == 1.748),
     adj = -0.5, col = "firebrick")
abline(v = log(lambda), col = "firebrick", lwd = 2)


Ridge_opt <- glmnet(x = XTrain,
                    y = YTrain,
                    alpha = 0,
                    lambda = Ridge_CV$lambda.min,
                    family="binomial")

Ridge_1se <- glmnet(x = XTrain,
                    y = YTrain,
                    alpha = 0,
                    lambda = Ridge_CV$lambda.1se,
                    family="binomial")

```

## Lasso Regression
Finally, Lasso Regression, which utilizes $L_1$ norm penalty, was conducted using the training dataset. It can be seen at figure below,  with increasing of $\lambda$ the estimates shrunk towards zero, so it can be used as feature selection. Cross-validation determined the optimal $\lambda$ to be 0.06684, with a larger value of $\lambda$ equal to 0.08434, within one standard deviation of the optimal, also considered for comparison. 

```{r lasso, echo=FALSE}
# Model fitting
Lasso <- glmnet(x = XTrain,
                y = YTrain,
                alpha = 1,
                family="binomial")

# Cross-validation
set.seed(1234)
Lasso_CV <- cv.glmnet(x = XTrain,
                      y = YTrain,
                      alpha = 1,
                      type.measure = "class",
                      family = "binomial") 

plot(Lasso_CV)
plot(Lasso_CV$glmnet.fit, xvar = "lambda",  xlim = c(-6,-0.5))
lambdal <- Lasso_CV$lambda.min
text(log(lambdal), -0.05, labels = expression(lambda == 0.06684),
     adj = -0.5, col = "firebrick")
abline(v = log(lambdal), col = "firebrick", lwd = 2)

Lasso_opt <- glmnet(x = XTrain,
                    y = YTrain,
                    alpha = 1,
                    lambda = Lasso_CV$lambda.min,
                    family="binomial")
qplot(
  summary(coef(Lasso_opt))[-1,1],
  summary(coef(Lasso_opt))[-1,3]) +
  xlab("gene ID") +
  ylab("ER") +
  geom_hline(yintercept = 0, color = "red")

Lasso_1se <- glmnet(x = XTrain,
                    y = YTrain,
                    alpha = 1,
                    lambda = Lasso_CV$lambda.1se,
                    family="binomial")

```
The model with the optimal $\lambda$ retains 17 predictors with non-zero coefficients, signifying the involvement of 17 genes. These genes are illustrated in the figure above, and their names are shown below.
```{r genes_opt, echo=FALSE}
Lasso_opt[["beta"]]@Dimnames[[1]][Lasso_opt[["beta"]]@i]
```

 In contrast, with the $\lambda$ set within one standard deviation of the optimal, the model includes only 12 genes, whose names are:
```{r genes_1se, echo=FALSE}
Lasso_1se[["beta"]]@Dimnames[[1]][Lasso_1se[["beta"]]@i]
```
 
## Evaluation and comparison of the models on test data
After constructing the optimal models which include PCR, Ridge Regression with optimal $\lambda$ (Ridge_opt), Ridge Regression with a $\lambda$ within one standard error (Ridge_1se), Lasso Regression with optimal $\lambda$ (Lasso_opt), and Lasso Regression with a $\lambda$ within one standard error (Lasso_1se), their performance was evaluated using the test dataset. The primary metric for this assessment was the misclassification error with a threshold being 0.5. The results indicated low misclassification errors for each model: PCR, Ridge_opt, and Lasso_1se all showed 4.5% error rate, corresponding to 5 incorrect predictions out of 101 test data points. Meanwhile, Ridge_1se and Lasso_opt demonstrated a slightly better performance at 3.9% error rate, equating to 4 misclassifications in the same 101 test data points. It is worth noting that each model presents a unique set of strengths and limitations. PCR is effective in handling multicollinearity but may lack in direct interpretability. Ridge Regression mitigates overfitting and maintains a comprehensive view by including all predictors, though this can make the model less parsimonious. Lasso Regression stands out for its feature selection capability, leading to more interpretable models but requires careful tuning to avoid excluding important variables. 

```{r comparison, echo=FALSE}
# PCR: performance on test data
# Transform test features on fitted on train features pca
pca_X_test <- data.frame(predict(pca_X, newdata=XTest))

pcr_perf <- predict(final_pcr_model, newdata = pca_X_test,
                    type = "response")
pcr_misclassification_error <- 
    misclass_error(YTest, pcr_perf)


# Ridge: performance on test data
ridge_preds <- predict(Ridge_opt,  s = Ridge_CV$lambda.min,
             newx = XTest, type = "response")

ridge_misclassification_error_opt <- 
    misclass_error(YTest, ridge_preds)

ridge_preds_1se <- predict(Ridge_1se,  s = Ridge_CV$lambda.1se,
                       newx = XTest, type = "response")
ridge_misclassification_error_1se <- 
    misclass_error(YTest, ridge_preds_1se)
 

# Lasso: performance on test data
lasso_preds_opt <- predict(Lasso_opt,  s = Lasso_CV$lambda.min,
                           newx = XTest, type = "response")
lasso_misclassification_error_opt <- 
    misclass_error(YTest, lasso_preds_opt)

lasso_preds_1se <- predict(Lasso_1se,  s = Lasso_CV$lambda.1se,
                           newx = XTest, type = "response")
lasso_misclassification_error_1se <- 
    misclass_error(YTest, lasso_preds_1se)
```

To further illustrate the performance of the models, we created a violin plot that displays the predicted probabilities of ER status from different predictive models alongside the actual outcomes in test dataset. The x-axis shows the actual ER status, categorized as 0 (absence of ER) or 1 (presence of ER), and the predictive models used for each status. The y-axis represents the predicted probability for the ER status being 1. A violin shape at each category of actual ER status represents the kernel density estimation of the predicted probabilities for that category, showing the distribution and concentration of predictions made by each model. The red points represent misclassified instances, where the model's prediction did not match the actual ER status. These are plotted at the predicted probability levels for visualization. The spread of the violin plots indicates the range and density of the predicted probabilities. A narrower plot signifies less variance in predictions, while a wider plot indicates greater variance. The alignment of the violins and points across the models allows for a direct comparison of prediction distributions and misclassification rates between them. The graph also serves as a visual comparison between models, highlighting their predictive behaviors.

```{r violin, echo=FALSE}
#Given that the error counts are consistent (4 or 5 errors out of 101  data points) across models, it is informative to visualize the 
#distributions of predicted probabilities compared to the true values.


# Combine predictions and actual outcomes into a data frame
pcr_data_opt <- data.frame(
  Prediction = pcr_perf,
  Actual = YTest)

ridge_data_opt <- data.frame(
  Prediction = ridge_preds[,1],
  Actual = YTest)

ridge_data_1se <- data.frame(
  Prediction = ridge_preds_1se[,1],
  Actual = YTest)

lasso_data_opt <- data.frame(
  Prediction = lasso_preds_opt[,1],
  Actual = YTest)

lasso_data_1se <- data.frame(
  Prediction = lasso_preds_1se[,1],
  Actual = YTest)

combined_data <- rbind(
  cbind(Model = "PCR", pcr_data_opt),
  cbind(Model = "Ridge_opt", ridge_data_opt),
  cbind(Model = "Ridge_1se", ridge_data_1se),
  cbind(Model = "Lasso_opt", lasso_data_opt),
  cbind(Model = "Lasso_1se", lasso_data_1se))


# Add a misclassification indicator to the data frame
combined_data$Misclassified <- (combined_data$Prediction > 0.5 & combined_data$Actual == 0) |
  (combined_data$Prediction < 0.5 & combined_data$Actual == 1)

ggplot(combined_data, aes(x = interaction(Actual, Model), y = Prediction, fill = Model)) +
  geom_violin() +
  geom_point(data = subset(combined_data, Misclassified),  
             aes(x = interaction(Actual, Model), y = Prediction, color = Misclassified), 
             size = 3, alpha = 0.6) +  
  geom_hline(yintercept=0.5, color="red", linetype="dashed", alpha=0.6) +
  scale_fill_manual(values = c("Lasso_1se" = "grey", "Lasso_opt" = "lightgreen",
                               "PCR" = "lightblue", "Ridge_1se" = "pink", "Ridge_opt" = "yellow")) +
  scale_color_manual(values = c("TRUE" = "red")) +  
  labs(title = "Comparison of Predicted Probabilities and True Values Across Models",
       x = "Actual ER Status and Models",
       y = "Predicted Probability") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10)) +
  guides(fill = guide_legend(title = "Model"), 
         color = guide_legend(title = "Misclassified"))  

```
To sum up, the ER status can be predicted from the gene expression data with low misclassification errors, and the selection of the appropriate model depends on the specific goal of the study as well as requirements and nuances of the dataset in question.

